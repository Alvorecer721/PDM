{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. llama 3 tokenizer\n",
    "2. Batch mapping\n",
    "3. Change model max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuyixuan/opt/anaconda3/envs/goldfish/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to batch tokenize a sequence of articles\n",
    "def batch_tokenize_gutenberg(batch, _tokenizer, char_pos_start=10_000, char_pos_end=70_000):\n",
    "    \"\"\"\n",
    "    Tokenize sequences from a batch of articles between specified character positions.\n",
    "    \n",
    "    Args:\n",
    "        batch (dict): Batch of data containing the 'text' field.\n",
    "        _tokenizer (AutoTokenizer): The tokenizer used for tokenization.\n",
    "        char_pos_start (int, optional): Starting character position. Defaults to 10_000.\n",
    "        char_pos_end (int, optional): Ending character position. Defaults to 70_000.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing input_ids and sequence lengths.\n",
    "    \"\"\"\n",
    "    input_ids_list = []\n",
    "    seq_length_list = []\n",
    "\n",
    "    for sequence in batch['text']:\n",
    "        # Slice and tokenize the sequence\n",
    "        input_ids = _tokenizer(sequence[char_pos_start:char_pos_end], truncation=False, padding=False).input_ids\n",
    "        input_ids_list.append(input_ids)\n",
    "        seq_length_list.append(len(input_ids))\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids_list,\n",
    "        'seq_length': seq_length_list\n",
    "    }\n",
    "\n",
    "\n",
    "# Function to select a random sequence of tokens and detokenize\n",
    "def select_tokens_from_random_offset(batch, _tokenizer, num_tokens=8000):\n",
    "    \"\"\"\n",
    "    Select a sequence of tokens with random offset from each batch, detokenize them, and return the results.\n",
    "    \n",
    "    Args:\n",
    "        batch (dict): Batch of data containing 'input_ids'.\n",
    "        _tokenizer (AutoTokenizer): The tokenizer used for detokenization.\n",
    "        num_tokens (int, optional): Number of tokens to extract. Defaults to 8000.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing selected tokens and detokenized texts.\n",
    "    \"\"\"\n",
    "    selected_tokens = []\n",
    "    detokenized_texts = []\n",
    "\n",
    "    # For each sequence of input_ids (tokenized data)\n",
    "    for input_ids in batch['input_ids']:\n",
    "        # Randomly select a starting offset ensuring at least num_tokens after it\n",
    "        offset = random.randint(0, len(input_ids) - num_tokens)\n",
    "\n",
    "        # Select the specified number of tokens starting from the random offset\n",
    "        selected_ids = input_ids[offset:offset + num_tokens]\n",
    "        selected_tokens.append(selected_ids)\n",
    "\n",
    "        # Detokenize the selected tokens to get the original text\n",
    "        detokenized_text = _tokenizer.decode(selected_ids, skip_special_tokens=True)\n",
    "        detokenized_texts.append(detokenized_text)\n",
    "\n",
    "    return {\n",
    "        'selected_tokens': selected_tokens,\n",
    "        'detokenized_texts': detokenized_texts\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and tokenizer\n",
    "ds = load_dataset(\"manu/project_gutenberg\", split=\"en\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "\n",
    "# Set the tokenizer's model max length\n",
    "tokenizer.model_max_length = 200_000\n",
    "\n",
    "# Define constants\n",
    "MIN_CHAR_LEN = 70_000\n",
    "NUM_ARTICLES = 10_000\n",
    "NUM_TOKENS = 9000\n",
    "\n",
    "# Filter and select the subset of articles\n",
    "subset = ds.filter(lambda example: len(example['text']) >= MIN_CHAR_LEN)\\\n",
    "           .select(range(NUM_ARTICLES))  # Here we select the first 10k articles\n",
    "\n",
    "# Tokenize the selected subset of articles\n",
    "gutenberg = subset.map(\n",
    "    batch_tokenize_gutenberg,\n",
    "    batched=True,\n",
    "    desc=\"Tokenizing Gutenberg English articles\",\n",
    "    num_proc=10,\n",
    "    fn_kwargs={\n",
    "        '_tokenizer': tokenizer, \n",
    "        'char_pos_start': 10_000, \n",
    "        'char_pos_end': MIN_CHAR_LEN\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "# Apply random token selection and detokenization\n",
    "gutenberg_with_8k_tokens = gutenberg.map(\n",
    "    select_tokens_from_random_offset,\n",
    "    batched=True,\n",
    "    desc=\"Selecting 8k tokens from random offset and detokenizing\",\n",
    "    num_proc=10,\n",
    "    fn_kwargs={'_tokenizer': tokenizer, 'num_tokens': NUM_TOKENS}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the results to JSONL, keeping only 'detokenized_texts'\n",
    "gutenberg_with_8k_tokens.select_columns(['detokenized_texts']).to_json('gutenberg_en_8k_tokens.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = '/users/xyixuan/store/datasets_raw/fineweb-sample-3BT/sample/3BT/000_00000.parquet'\n",
    "\n",
    "# 1. Read just the first few rows\n",
    "df_peek = pd.read_parquet(file_path, nrows=5)\n",
    "print(\"First 5 rows:\")\n",
    "print(df_peek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goldfish",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
