{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/iopsstor/scratch/cscs/xyixuan/Megatron-LM\")\n",
    "sys.path.append(\"/capstor/users/cscs/xyixuan/PDM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "import torch\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint_path='/iopsstor/scratch/cscs/xyixuan/Megatron-LM/logs/Meg-Runs/Goldfish/llama3-1b-15n-8192sl-60gbsz-goldfish-no-bos/torch/iter_0170005/mp_rank_00/model_optim_rng.pt'\n",
    "checkpoint = torch.load(checkpoint_path, weights_only=False, map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = checkpoint['args']\n",
    "\n",
    "config = AutoConfig.for_model(\n",
    "    architectures=[\"LlamaForCausalLM\"],\n",
    "    attention_bias=False,\n",
    "    attention_dropout=args.attention_dropout,\n",
    "    bos_token_id=128000,\n",
    "    eos_token_id=128001,\n",
    "    head_dim=int(args.hidden_size/args.num_attention_heads),\n",
    "    hidden_act=\"silu\",\n",
    "    hidden_size=args.hidden_size,\n",
    "    initializer_range=0.01,\n",
    "    intermediate_size=args.ffn_hidden_size,\n",
    "    max_position_embeddings=131072,\n",
    "    mlp_bias=False,\n",
    "    model_type=\"llama\",\n",
    "    num_attention_heads=args.num_attention_heads,\n",
    "    num_hidden_layers=args.num_layers,\n",
    "    num_key_value_heads=args.num_query_groups,\n",
    "    pretraining_tp=1,\n",
    "    rms_norm_eps=args.norm_epsilon,\n",
    "    rope_scaling={\n",
    "        \"factor\": args.rope_scaling_factor,\n",
    "        \"high_freq_factor\": 4.0,\n",
    "        \"low_freq_factor\": 1.0,\n",
    "        \"original_max_position_embeddings\": args.max_position_embeddings,\n",
    "        \"rope_type\": \"llama3\"\n",
    "    },\n",
    "    rope_theta=args.rotary_base,\n",
    "    tie_word_embeddings=not args.untie_embeddings_and_output_weights,\n",
    "    torch_dtype=args.params_dtype,\n",
    "    use_cache=True,\n",
    "    vocab_size=args.padded_vocab_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done layer 0\n",
      "Done layer 1\n",
      "Done layer 2\n",
      "Done layer 3\n",
      "Done layer 4\n",
      "Done layer 5\n",
      "Done layer 6\n",
      "Done layer 7\n",
      "Done layer 8\n",
      "Done layer 9\n",
      "Done layer 10\n",
      "Done layer 11\n",
      "Done layer 12\n",
      "Done layer 13\n",
      "Done layer 14\n",
      "Done layer 15\n"
     ]
    }
   ],
   "source": [
    "def convert_megatron_to_hf_dict(model_dict):\n",
    "    \"\"\"\n",
    "    Implementation adopted from https://github.com/TJ-Solergibert/NeMo/blob/825c246b12e76ee7e9b3cdf01aea9c9dacdc03fe/scripts/checkpoint_converters/convert_llama_nemo_to_hf.py#L106\n",
    "    ALERT: Currently no support for model parallelism\n",
    "    \"\"\"\n",
    "    from collections import OrderedDict \n",
    "    checkpoint = OrderedDict()  # using OrderedDict for consistent ordering\n",
    "    \n",
    "    # Get model dimensions\n",
    "    hidden_size = model_dict['decoder.layers.0.self_attention.linear_qkv.weight'].shape[1]\n",
    "    head_num = args.num_attention_heads\n",
    "    num_query_groups = args.num_query_groups\n",
    "    \n",
    "    # Calculate attention dimensions\n",
    "    head_size = hidden_size // head_num\n",
    "    heads_per_group = head_num // num_query_groups\n",
    "    qkv_total_dim = head_num + 2 * num_query_groups\n",
    "\n",
    "    # Save embedding\n",
    "    checkpoint['model.embed_tokens.weight'] = model_dict['embedding.word_embeddings.weight']\n",
    "\n",
    "    # Process each transformer layer\n",
    "    for layer_idx in range(args.num_layers):\n",
    "        # Handle QKV weights\n",
    "        qkv_weights = model_dict[f'decoder.layers.{layer_idx}.self_attention.linear_qkv.weight']\n",
    "        qkv_weights = qkv_weights.reshape([qkv_total_dim, head_size, hidden_size])\n",
    "\n",
    "        # Calculate indices for Q, K, V separation\n",
    "        q_slice = torch.cat([\n",
    "            torch.arange((heads_per_group + 2) * i, (heads_per_group + 2) * i + heads_per_group)\n",
    "            for i in range(num_query_groups)\n",
    "        ])\n",
    "        k_slice = torch.arange(heads_per_group, qkv_total_dim, (heads_per_group + 2))\n",
    "        v_slice = torch.arange(heads_per_group + 1, qkv_total_dim, (heads_per_group + 2))\n",
    "\n",
    "        # Separate and save Q, K, V weights\n",
    "        checkpoint[f'model.layers.{layer_idx}.self_attn.q_proj.weight'] = qkv_weights[q_slice].reshape(-1, hidden_size)\n",
    "        checkpoint[f'model.layers.{layer_idx}.self_attn.k_proj.weight'] = qkv_weights[k_slice].reshape(-1, hidden_size)\n",
    "        checkpoint[f'model.layers.{layer_idx}.self_attn.v_proj.weight'] = qkv_weights[v_slice].reshape(-1, hidden_size)\n",
    "\n",
    "        # Save attention output projection\n",
    "        checkpoint[f'model.layers.{layer_idx}.self_attn.o_proj.weight'] = model_dict[f'decoder.layers.{layer_idx}.self_attention.linear_proj.weight']\n",
    "\n",
    "        # Handle MLP weights\n",
    "        mlp_weight = model_dict[f'decoder.layers.{layer_idx}.mlp.linear_fc1.weight']\n",
    "        ffn_hidden_size = mlp_weight.shape[0] // 2\n",
    "        checkpoint[f'model.layers.{layer_idx}.mlp.gate_proj.weight'] = mlp_weight[:ffn_hidden_size, :]\n",
    "        checkpoint[f'model.layers.{layer_idx}.mlp.up_proj.weight'] = mlp_weight[ffn_hidden_size:, :]\n",
    "        checkpoint[f'model.layers.{layer_idx}.mlp.down_proj.weight'] = model_dict[f'decoder.layers.{layer_idx}.mlp.linear_fc2.weight']\n",
    "\n",
    "        # Save layer norms\n",
    "        checkpoint[f'model.layers.{layer_idx}.input_layernorm.weight'] = model_dict[f'decoder.layers.{layer_idx}.self_attention.linear_qkv.layer_norm_weight']\n",
    "        checkpoint[f'model.layers.{layer_idx}.post_attention_layernorm.weight'] = model_dict[f'decoder.layers.{layer_idx}.mlp.linear_fc1.layer_norm_weight']\n",
    "\n",
    "        print(f\"Done layer {layer_idx}\")\n",
    "\n",
    "    # Save final layer norm\n",
    "    checkpoint['model.norm.weight'] = model_dict['decoder.final_layernorm.weight']\n",
    "    \n",
    "    # Handle output layer (weight tying if needed)\n",
    "    if not args.untie_embeddings_and_output_weights:\n",
    "        checkpoint['lm_head.weight'] = checkpoint['model.embed_tokens.weight']\n",
    "    else:\n",
    "        checkpoint['lm_head.weight'] = model_dict['output_layer.weight']\n",
    "\n",
    "    return checkpoint\n",
    "\n",
    "# Convert and load the model\n",
    "model_dict = checkpoint['model']\n",
    "hf_dict = convert_megatron_to_hf_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_config(config)\n",
    "\n",
    "# Load the state dict\n",
    "# You might need to process the state dict to match transformer's format\n",
    "model.load_state_dict(hf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 1,235,814,400\n"
     ]
    }
   ],
   "source": [
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.infer.convert_megatron_to_hf import convert_megatron_checkpoint_to_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Arguments:\n",
      "================================================================================\n",
      "num_layers........................................                            16\n",
      "encoder_num_layers................................                            16\n",
      "decoder_num_layers................................                          None\n",
      "hidden_size.......................................                          2048\n",
      "ffn_hidden_size...................................                          8192\n",
      "num_attention_heads...............................                            32\n",
      "attention_backend.................................              AttnBackend.auto\n",
      "kv_channels.......................................                            64\n",
      "group_query_attention.............................                          True\n",
      "num_query_groups..................................                             8\n",
      "max_position_embeddings...........................                          8192\n",
      "position_embedding_type...........................                          rope\n",
      "relative_attention_num_buckets....................                            32\n",
      "relative_attention_max_distance...................                           128\n",
      "use_rotary_position_embeddings....................                         False\n",
      "rotary_base.......................................                        500000\n",
      "rotary_percent....................................                           1.0\n",
      "rotary_interleaved................................                         False\n",
      "rotary_seq_len_interpolation_factor...............                          None\n",
      "use_rope_scaling..................................                          True\n",
      "rope_scaling_factor...............................                          32.0\n",
      "add_position_embedding............................                          True\n",
      "make_vocab_size_divisible_by......................                           128\n",
      "normalization.....................................                       RMSNorm\n",
      "norm_epsilon......................................                         1e-05\n",
      "apply_layernorm_1p................................                         False\n",
      "apply_residual_connection_post_layernorm..........                         False\n",
      "openai_gelu.......................................                         False\n",
      "squared_relu......................................                         False\n",
      "swiglu............................................                          True\n",
      "onnx_safe.........................................                          None\n",
      "bert_binary_head..................................                          True\n",
      "untie_embeddings_and_output_weights...............                         False\n",
      "multi_latent_attention............................                         False\n",
      "attention_dropout.................................                           0.0\n",
      "hidden_dropout....................................                           0.0\n",
      "weight_decay......................................                           0.1\n",
      "start_weight_decay................................                           0.1\n",
      "end_weight_decay..................................                           0.1\n",
      "weight_decay_incr_style...........................                      constant\n",
      "clip_grad.........................................                           1.0\n",
      "adam_beta1........................................                           0.9\n",
      "adam_beta2........................................                          0.95\n",
      "adam_eps..........................................                         1e-08\n",
      "sgd_momentum......................................                           0.9\n",
      "micro_batch_size..................................                             1\n",
      "global_batch_size.................................                            60\n",
      "rampup_batch_size.................................                          None\n",
      "decrease_batch_size_if_needed.....................                         False\n",
      "recompute_granularity.............................                          None\n",
      "check_for_nan_in_loss_and_grad....................                         False\n",
      "check_for_spiky_loss..............................                         False\n",
      "distribute_saved_activations......................                         False\n",
      "recompute_method..................................                          None\n",
      "recompute_num_layers..............................                          None\n",
      "clone_scatter_output_in_embedding.................                          True\n",
      "profile...........................................                         False\n",
      "profile_step_start................................                            10\n",
      "profile_step_end..................................                            12\n",
      "use_pytorch_profiler..............................                         False\n",
      "profile_ranks.....................................                           [0]\n",
      "record_memory_history.............................                         False\n",
      "memory_snapshot_path..............................               snapshot.pickle\n",
      "tp_comm_overlap...................................                         False\n",
      "tp_comm_overlap_cfg...............................                          None\n",
      "tp_comm_overlap_ag................................                          True\n",
      "tp_comm_overlap_rs................................                          True\n",
      "tp_comm_overlap_rs_dgrad..........................                         False\n",
      "tp_comm_bulk_dgrad................................                          True\n",
      "tp_comm_bulk_wgrad................................                          True\n",
      "tp_comm_bootstrap_backend.........................                          nccl\n",
      "use_cpu_initialization............................                          None\n",
      "empty_unused_memory_level.........................                             0\n",
      "deterministic_mode................................                         False\n",
      "check_weight_hash_across_dp_replicas_interval.....                          None\n",
      "calculate_per_token_loss..........................                         False\n",
      "train_sync_interval...............................                          None\n",
      "train_iters.......................................                        170005\n",
      "train_samples.....................................                          None\n",
      "log_interval......................................                             1\n",
      "exit_interval.....................................                          None\n",
      "exit_duration_in_mins.............................                          None\n",
      "exit_signal_handler...............................                         False\n",
      "tensorboard_dir...................................\n",
      "                    /iopsstor/scratch/cscs/xyixuan/Megatron-LM/logs/Meg-Runs/Gol\n",
      "                    dfish/llama3-1b-15n-8192sl-60gbsz-goldfish-no-bos/logging/te\n",
      "                    nsorboard\n",
      "masked_softmax_fusion.............................                          True\n",
      "bias_gelu_fusion..................................                         False\n",
      "bias_swiglu_fusion................................                          True\n",
      "bias_dropout_fusion...............................                          True\n",
      "apply_rope_fusion.................................                          True\n",
      "cross_entropy_loss_fusion.........................                          True\n",
      "use_flash_attn....................................                         False\n",
      "add_bias_linear...................................                         False\n",
      "add_qkv_bias......................................                         False\n",
      "optimizer.........................................                          adam\n",
      "dataloader_type...................................                        single\n",
      "async_tensor_model_parallel_allreduce.............                          True\n",
      "no_persist_layer_norm.............................                         False\n",
      "sequence_parallel.................................                         False\n",
      "gradient_accumulation_fusion......................                          True\n",
      "deprecated_use_mcore_models.......................                         False\n",
      "use_legacy_models.................................                         False\n",
      "manual_gc.........................................                          True\n",
      "manual_gc_interval................................                          5000\n",
      "manual_gc_eval....................................                          True\n",
      "tp_comm_split_ag..................................                          True\n",
      "tp_comm_split_rs..................................                          True\n",
      "seed..............................................                            28\n",
      "data_parallel_random_init.........................                         False\n",
      "init_method_std...................................                      0.008944\n",
      "init_method_xavier_uniform........................                         False\n",
      "lr................................................                        0.0003\n",
      "lr_decay_style....................................                        cosine\n",
      "lr_wsd_decay_style................................                        1-sqrt\n",
      "lr_decay_iters....................................                        170005\n",
      "lr_decay_samples..................................                          None\n",
      "lr_wsd_decay_samples..............................                          None\n",
      "lr_wsd_decay_iters................................                          None\n",
      "lr_warmup_fraction................................                          None\n",
      "lr_warmup_iters...................................                          2000\n",
      "lr_warmup_samples.................................                             0\n",
      "lr_warmup_init....................................                           0.0\n",
      "min_lr............................................                         3e-05\n",
      "override_opt_param_scheduler......................                         False\n",
      "use_checkpoint_opt_param_scheduler................                         False\n",
      "decoupled_lr......................................                          None\n",
      "decoupled_min_lr..................................                          None\n",
      "save..............................................\n",
      "                    /iopsstor/scratch/cscs/xyixuan/Megatron-LM/logs/Meg-Runs/Gol\n",
      "                    dfish/llama3-1b-15n-8192sl-60gbsz-goldfish-no-bos/ckpt-torch\n",
      "                    /torch\n",
      "save_interval.....................................                         15455\n",
      "no_save_optim.....................................                          None\n",
      "no_save_rng.......................................                          None\n",
      "load..............................................\n",
      "                    /iopsstor/scratch/cscs/xyixuan/Megatron-LM/logs/Meg-Runs/Gol\n",
      "                    dfish/llama3-1b-15n-8192sl-60gbsz-goldfish-no-bos/checkpoint\n",
      "                    s\n",
      "no_load_optim.....................................                          None\n",
      "no_load_rng.......................................                          None\n",
      "non_persistent_save_interval......................                          None\n",
      "non_persistent_ckpt_type..........................                          None\n",
      "non_persistent_global_ckpt_dir....................                          None\n",
      "non_persistent_local_ckpt_dir.....................                          None\n",
      "non_persistent_local_ckpt_algo....................                fully_parallel\n",
      "finetune..........................................                         False\n",
      "pretrained_checkpoint.............................                          None\n",
      "ckpt_step.........................................                          None\n",
      "perform_initialization............................                          True\n",
      "use_checkpoint_args...............................                         False\n",
      "use_mp_args_from_checkpoint_args..................                         False\n",
      "use_tokenizer_model_from_checkpoint_args..........                          True\n",
      "exit_on_missing_checkpoint........................                          True\n",
      "use_dist_ckpt_deprecated..........................                         False\n",
      "auto_detect_ckpt_format...........................                         False\n",
      "dist_ckpt_format_deprecated.......................                          None\n",
      "ckpt_format.......................................                         torch\n",
      "ckpt_convert_format...............................                         torch\n",
      "ckpt_convert_save.................................\n",
      "                    /iopsstor/scratch/cscs/xyixuan/Megatron-LM/logs/Meg-Runs/Gol\n",
      "                    dfish/llama3-1b-15n-8192sl-60gbsz-goldfish-no-bos/ckpt-torch\n",
      "ckpt_convert_update_legacy_dist_opt_format........                         False\n",
      "ckpt_fully_parallel_save_deprecated...............                         False\n",
      "ckpt_fully_parallel_save..........................                          True\n",
      "async_save........................................                          None\n",
      "ckpt_fully_parallel_load..........................                         False\n",
      "ckpt_assume_constant_structure....................                         False\n",
      "dist_ckpt_strictness..............................          assume_ok_unexpected\n",
      "fp16..............................................                         False\n",
      "bf16..............................................                          True\n",
      "loss_scale........................................                          None\n",
      "initial_loss_scale................................                    4294967296\n",
      "min_loss_scale....................................                           1.0\n",
      "loss_scale_window.................................                          1000\n",
      "hysteresis........................................                             2\n",
      "fp32_residual_connection..........................                         False\n",
      "apply_query_key_layer_scaling.....................                         False\n",
      "attention_softmax_in_fp32.........................                         False\n",
      "accumulate_allreduce_grads_in_fp32................                         False\n",
      "fp16_lm_cross_entropy.............................                         False\n",
      "tensor_model_parallel_size........................                             1\n",
      "encoder_tensor_model_parallel_size................                             0\n",
      "pipeline_model_parallel_size......................                             1\n",
      "encoder_pipeline_model_parallel_size..............                             0\n",
      "pipeline_model_parallel_split_rank................                          None\n",
      "decoder_first_pipeline_num_layers.................                          None\n",
      "decoder_last_pipeline_num_layers..................                          None\n",
      "num_layers_per_virtual_pipeline_stage.............                          None\n",
      "num_virtual_stages_per_pipeline_rank..............                          None\n",
      "microbatch_group_size_per_vp_stage................                          None\n",
      "overlap_p2p_comm..................................                         False\n",
      "overlap_p2p_comm_warmup_flush.....................                         False\n",
      "distributed_backend...............................                          nccl\n",
      "distributed_timeout_minutes.......................                            10\n",
      "overlap_grad_reduce...............................                          True\n",
      "defer_embedding_wgrad_compute.....................                         False\n",
      "wgrad_deferral_limit..............................                            50\n",
      "align_grad_reduce.................................                          True\n",
      "ddp_bucket_size...................................                          None\n",
      "ddp_average_in_collective.........................                         False\n",
      "overlap_param_gather..............................                          True\n",
      "overlap_param_gather_with_optimizer_step..........                         False\n",
      "align_param_gather................................                         False\n",
      "scatter_gather_tensors_in_pipeline................                          True\n",
      "use_ring_exchange_p2p.............................                         False\n",
      "local_rank........................................                             0\n",
      "lazy_mpu_init.....................................                          None\n",
      "account_for_embedding_in_pipeline_split...........                         False\n",
      "account_for_loss_in_pipeline_split................                         False\n",
      "use_distributed_optimizer.........................                          True\n",
      "num_distributed_optimizer_instances...............                             1\n",
      "use_torch_fsdp2...................................                         False\n",
      "context_parallel_size.............................                             1\n",
      "cp_comm_type......................................                       ['p2p']\n",
      "hierarchical_context_parallel_sizes...............                          None\n",
      "nccl_communicator_config_path.....................                          None\n",
      "use_tp_pp_dp_mapping..............................                         False\n",
      "replication.......................................                         False\n",
      "replication_jump..................................                          None\n",
      "replication_factor................................                             2\n",
      "eval_iters........................................                           100\n",
      "eval_interval.....................................                          1000\n",
      "test_mode.........................................                         False\n",
      "skip_train........................................                         False\n",
      "data_path.........................................\n",
      "                    ['/capstor/scratch/cscs/xyixuan/fineweb-edu-80B/finewebedu-l\n",
      "                    lama3tok_text_document', '/iopsstor/scratch/cscs/xyixuan/dat\n",
      "                    aset/sparse_gutenberg/rep_1/00000_tokens', '/iopsstor/scratc\n",
      "                    h/cscs/xyixuan/dataset/sparse_gutenberg/rep_2/00000_tokens',\n",
      "                     '/iopsstor/scratch/cscs/xyixuan/dataset/sparse_gutenberg/re\n",
      "                    p_3/00000_tokens', '/iopsstor/scratch/cscs/xyixuan/dataset/s\n",
      "                    parse_gutenberg/rep_4/00000_tokens', '/iopsstor/scratch/cscs\n",
      "                    /xyixuan/dataset/sparse_gutenberg/rep_8/00000_tokens', '/iop\n",
      "                    sstor/scratch/cscs/xyixuan/dataset/sparse_gutenberg/rep_16/0\n",
      "                    0000_tokens', '/iopsstor/scratch/cscs/xyixuan/dataset/sparse\n",
      "                    _gutenberg/rep_24/00000_tokens', '/iopsstor/scratch/cscs/xyi\n",
      "                    xuan/dataset/sparse_gutenberg/rep_32/00000_tokens', '/iopsst\n",
      "                    or/scratch/cscs/xyixuan/dataset/sparse_gutenberg/rep_48/0000\n",
      "                    0_tokens', '/iopsstor/scratch/cscs/xyixuan/dataset/sparse_gu\n",
      "                    tenberg/rep_64/00000_tokens', '/iopsstor/scratch/cscs/xyixua\n",
      "                    n/dataset/sparse_gutenberg/rep_96/00000_tokens', '/iopsstor/\n",
      "                    scratch/cscs/xyixuan/dataset/sparse_gutenberg/rep_128/00000_\n",
      "                    tokens']\n",
      "split.............................................                       100,0,0\n",
      "train_data_path...................................                          None\n",
      "valid_data_path...................................                          None\n",
      "test_data_path....................................                          None\n",
      "data_args_path....................................                          None\n",
      "per_split_data_args_path..........................                          None\n",
      "data_cache_path.................................../iopsstor/scratch/cscs/xyixuan/datasets/cache\n",
      "mmap_bin_files....................................                          True\n",
      "mock_data.........................................                         False\n",
      "seq_length........................................                          8192\n",
      "encoder_seq_length................................                          8192\n",
      "decoder_seq_length................................                          None\n",
      "retriever_seq_length..............................                           256\n",
      "sample_rate.......................................                           1.0\n",
      "mask_prob.........................................                          0.15\n",
      "short_seq_prob....................................                           0.1\n",
      "num_workers.......................................                             2\n",
      "reset_position_ids................................                         False\n",
      "reset_attention_mask..............................                          True\n",
      "eod_mask_loss.....................................                         False\n",
      "bod_hiding........................................                          True\n",
      "goldfish_loss.....................................                          True\n",
      "goldfish_k........................................                            50\n",
      "goldfish_h........................................                            50\n",
      "create_attention_mask_in_dataloader...............                          True\n",
      "cross_document_attention..........................                          True\n",
      "num_dataset_builder_threads.......................                             1\n",
      "s3_cache_path.....................................                          None\n",
      "vocab_size........................................                          None\n",
      "vocab_file........................................                          None\n",
      "merge_file........................................                          None\n",
      "vocab_extra_ids...................................                             0\n",
      "tokenizer_type....................................          HuggingFaceTokenizer\n",
      "tokenizer_model...................................  nvidia/OpenMath2-Llama3.1-8B\n",
      "tiktoken_pattern..................................                          None\n",
      "tiktoken_num_special_tokens.......................                          1000\n",
      "tiktoken_special_tokens...........................                          None\n",
      "adlr_autoresume...................................                         False\n",
      "adlr_autoresume_interval..........................                          1000\n",
      "ict_head_size.....................................                          None\n",
      "biencoder_projection_dim..........................                             0\n",
      "biencoder_shared_query_context_model..............                         False\n",
      "ict_load..........................................                          None\n",
      "bert_load.........................................                          None\n",
      "titles_data_path..................................                          None\n",
      "query_in_block_prob...............................                           0.1\n",
      "use_one_sent_docs.................................                         False\n",
      "evidence_data_path................................                          None\n",
      "retriever_report_topk_accuracies..................                            []\n",
      "retriever_score_scaling...........................                         False\n",
      "block_data_path...................................                          None\n",
      "embedding_path....................................                          None\n",
      "indexer_batch_size................................                           128\n",
      "indexer_log_interval..............................                          1000\n",
      "num_classes.......................................                          1000\n",
      "img_h.............................................                           224\n",
      "img_w.............................................                           224\n",
      "num_channels......................................                             3\n",
      "patch_dim.........................................                            16\n",
      "classes_fraction..................................                           1.0\n",
      "data_per_class_fraction...........................                           1.0\n",
      "data_sharding.....................................                          True\n",
      "head_lr_mult......................................                           1.0\n",
      "vision_pretraining................................                         False\n",
      "vision_pretraining_type...........................                      classify\n",
      "vision_backbone_type..............................                           vit\n",
      "swin_backbone_type................................                          tiny\n",
      "mask_type.........................................                        random\n",
      "mask_factor.......................................                           1.0\n",
      "iter_per_epoch....................................                          1250\n",
      "dino_local_img_size...............................                            96\n",
      "dino_local_crops_number...........................                            10\n",
      "dino_head_hidden_size.............................                          2048\n",
      "dino_bottleneck_size..............................                           256\n",
      "dino_freeze_last_layer............................                             1\n",
      "dino_norm_last_layer..............................                         False\n",
      "dino_warmup_teacher_temp..........................                          0.04\n",
      "dino_teacher_temp.................................                          0.07\n",
      "dino_warmup_teacher_temp_epochs...................                            30\n",
      "qk_layernorm......................................                         False\n",
      "expert_model_parallel_size........................                             1\n",
      "expert_tensor_parallel_size.......................                             1\n",
      "num_experts.......................................                          None\n",
      "moe_layer_freq....................................                             1\n",
      "moe_ffn_hidden_size...............................                          8192\n",
      "moe_shared_expert_intermediate_size...............                          None\n",
      "moe_shared_expert_overlap.........................                         False\n",
      "moe_grouped_gemm..................................                         False\n",
      "moe_router_load_balancing_type....................                      aux_loss\n",
      "moe_router_score_function.........................                       softmax\n",
      "moe_router_topk...................................                             2\n",
      "moe_router_pre_softmax............................                         False\n",
      "moe_router_topk_limited_devices...................                          None\n",
      "moe_router_topk_scaling_factor....................                          None\n",
      "moe_router_enable_expert_bias.....................                         False\n",
      "moe_router_bias_update_rate.......................                         0.001\n",
      "moe_use_legacy_grouped_gemm.......................                         False\n",
      "moe_aux_loss_coeff................................                           0.0\n",
      "moe_z_loss_coeff..................................                          None\n",
      "moe_input_jitter_eps..............................                          None\n",
      "moe_token_dispatcher_type.........................                     allgather\n",
      "moe_per_layer_logging.............................                         False\n",
      "moe_expert_capacity_factor........................                          None\n",
      "moe_pad_expert_input_to_capacity..................                         False\n",
      "moe_token_drop_policy.............................                         probs\n",
      "moe_layer_recompute...............................                         False\n",
      "moe_extended_tp...................................                         False\n",
      "moe_use_upcycling.................................                         False\n",
      "q_lora_rank.......................................                          None\n",
      "kv_lora_rank......................................                            32\n",
      "qk_head_dim.......................................                           128\n",
      "qk_pos_emb_head_dim...............................                            64\n",
      "v_head_dim........................................                           128\n",
      "rotary_scaling_factor.............................                           1.0\n",
      "log_params_norm...................................                         False\n",
      "log_num_zeros_in_grad.............................                         False\n",
      "log_throughput....................................                          True\n",
      "log_progress......................................                          True\n",
      "timing_log_level..................................                             0\n",
      "barrier_with_L1_time..............................                          True\n",
      "timing_log_option.................................                        minmax\n",
      "tensorboard_log_interval..........................                             1\n",
      "tensorboard_queue_size............................                          1000\n",
      "log_timers_to_tensorboard.........................                          True\n",
      "log_loss_scale_to_tensorboard.....................                         False\n",
      "log_validation_ppl_to_tensorboard.................                         False\n",
      "log_memory_to_tensorboard.........................                          True\n",
      "log_world_size_to_tensorboard.....................                         False\n",
      "wandb_project.....................................                      Goldfish\n",
      "wandb_exp_name....................................llama3-1b-15n-8192sl-60gbsz-goldfish-no-bos-182467\n",
      "wandb_save_dir....................................\n",
      "                    /iopsstor/scratch/cscs/xyixuan/Megatron-LM/logs/Meg-Runs/Gol\n",
      "                    dfish/llama3-1b-15n-8192sl-60gbsz-goldfish-no-bos/logging\n",
      "logging_level.....................................                          None\n",
      "log_straggler.....................................                         False\n",
      "disable_straggler_on_startup......................                         False\n",
      "straggler_ctrlr_port..............................                         65535\n",
      "straggler_minmax_count............................                             1\n",
      "inference_batch_times_seqlen_threshold............                            -1\n",
      "max_tokens_to_oom.................................                         12000\n",
      "output_bert_embeddings............................                         False\n",
      "bert_embedder_type................................                      megatron\n",
      "flash_decode......................................                         False\n",
      "enable_cuda_graph.................................                         False\n",
      "cuda_graph_warmup_steps...........................                             2\n",
      "inference_max_seq_length..........................                          2560\n",
      "fp8...............................................                          None\n",
      "fp8_margin........................................                             0\n",
      "fp8_interval......................................                             1\n",
      "fp8_amax_history_len..............................                             1\n",
      "fp8_amax_compute_algo.............................                   most_recent\n",
      "fp8_wgrad.........................................                          True\n",
      "transformer_impl..................................            transformer_engine\n",
      "fp8_param_gather..................................                         False\n",
      "te_rng_tracker....................................                         False\n",
      "inference_rng_tracker.............................                         False\n",
      "retro_project_dir.................................                          None\n",
      "retro_add_retriever...............................                         False\n",
      "retro_cyclic_train_iters..........................                          None\n",
      "retro_encoder_layers..............................                             2\n",
      "retro_encoder_hidden_dropout......................                           0.1\n",
      "retro_encoder_attention_dropout...................                           0.1\n",
      "retro_num_neighbors...............................                             2\n",
      "retro_num_retrieved_chunks........................                             2\n",
      "retro_attention_gate..............................                             1\n",
      "retro_verify_neighbor_count.......................                          True\n",
      "spec..............................................                          None\n",
      "hybrid_attention_ratio............................                           0.0\n",
      "hybrid_mlp_ratio..................................                           0.0\n",
      "hybrid_override_pattern...........................                          None\n",
      "yaml_cfg..........................................                          None\n",
      "use_precision_aware_optimizer.....................                          True\n",
      "main_grads_dtype..................................                torch.bfloat16\n",
      "main_params_dtype.................................                 torch.float32\n",
      "exp_avg_dtype.....................................                 torch.float32\n",
      "exp_avg_sq_dtype..................................                 torch.float32\n",
      "enable_one_logger.................................                          True\n",
      "one_logger_project................................                   megatron-lm\n",
      "one_logger_run_name...............................                          None\n",
      "one_logger_async..................................                         False\n",
      "app_tag_run_name..................................                          None\n",
      "app_tag_run_version...............................                         0.0.0\n",
      "enable_ft_package.................................                         False\n",
      "calc_ft_timeouts..................................                         False\n",
      "config_logger_dir.................................                              \n",
      "error_injection_rate..............................                             0\n",
      "error_injection_type..............................               transient_error\n",
      "rerun_mode........................................                      disabled\n",
      "rank..............................................                             0\n",
      "world_size........................................                            60\n",
      "use_dist_ckpt.....................................                         False\n",
      "transformer_pipeline_model_parallel_size..........                             1\n",
      "data_parallel_size................................                            60\n",
      "virtual_pipeline_model_parallel_size..............                          None\n",
      "params_dtype......................................                torch.bfloat16\n",
      "consumed_train_samples............................                      10200300\n",
      "skipped_train_samples.............................                             0\n",
      "consumed_valid_samples............................                             0\n",
      "variable_seq_lengths..............................                         False\n",
      "padded_vocab_size.................................                        128256\n",
      "model_type........................................  ModelType.encoder_or_decoder\n",
      "iteration.........................................                        170005\n",
      "num_floating_point_operations_so_far..............         8.887287452918612e+20\n",
      "Number of trainable parameters: 1,235,814,400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_megatron_checkpoint_to_hf(\n",
    "    checkpoint_path='/iopsstor/scratch/cscs/xyixuan/Megatron-LM/logs/Meg-Runs/Goldfish/llama3-1b-15n-8192sl-60gbsz-goldfish-no-bos/torch/iter_0170005/mp_rank_00/model_optim_rng.pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_layers: 16\n",
      "encoder_num_layers: 16\n",
      "decoder_num_layers: None\n",
      "hidden_size: 2048\n",
      "ffn_hidden_size: 8192\n",
      "num_attention_heads: 32\n",
      "attention_backend: AttnBackend.auto\n",
      "kv_channels: 64\n",
      "group_query_attention: True\n",
      "num_query_groups: 8\n",
      "max_position_embeddings: 8192\n",
      "position_embedding_type: rope\n",
      "relative_attention_num_buckets: 32\n",
      "relative_attention_max_distance: 128\n",
      "use_rotary_position_embeddings: False\n",
      "rotary_base: 500000\n",
      "rotary_percent: 1.0\n",
      "rotary_interleaved: False\n",
      "rotary_seq_len_interpolation_factor: None\n",
      "use_rope_scaling: True\n",
      "rope_scaling_factor: 32.0\n",
      "add_position_embedding: True\n",
      "make_vocab_size_divisible_by: 128\n",
      "normalization: RMSNorm\n",
      "norm_epsilon: 1e-05\n",
      "apply_layernorm_1p: False\n",
      "apply_residual_connection_post_layernorm: False\n",
      "openai_gelu: False\n",
      "squared_relu: False\n",
      "swiglu: True\n",
      "onnx_safe: None\n",
      "bert_binary_head: True\n",
      "untie_embeddings_and_output_weights: False\n",
      "multi_latent_attention: False\n",
      "attention_dropout: 0.0\n",
      "hidden_dropout: 0.0\n",
      "weight_decay: 0.1\n",
      "start_weight_decay: 0.1\n",
      "end_weight_decay: 0.1\n",
      "weight_decay_incr_style: constant\n",
      "clip_grad: 1.0\n",
      "adam_beta1: 0.9\n",
      "adam_beta2: 0.95\n",
      "adam_eps: 1e-08\n",
      "sgd_momentum: 0.9\n",
      "micro_batch_size: 1\n",
      "global_batch_size: 60\n",
      "rampup_batch_size: None\n",
      "decrease_batch_size_if_needed: False\n",
      "recompute_granularity: None\n",
      "check_for_nan_in_loss_and_grad: False\n",
      "check_for_spiky_loss: False\n",
      "distribute_saved_activations: False\n",
      "recompute_method: None\n",
      "recompute_num_layers: None\n",
      "clone_scatter_output_in_embedding: True\n",
      "profile: False\n",
      "profile_step_start: 10\n",
      "profile_step_end: 12\n",
      "use_pytorch_profiler: False\n",
      "profile_ranks: [0]\n",
      "record_memory_history: False\n",
      "memory_snapshot_path: snapshot.pickle\n",
      "tp_comm_overlap: False\n",
      "tp_comm_overlap_cfg: None\n",
      "tp_comm_overlap_ag: True\n",
      "tp_comm_overlap_rs: True\n",
      "tp_comm_overlap_rs_dgrad: False\n",
      "tp_comm_bulk_dgrad: True\n",
      "tp_comm_bulk_wgrad: True\n",
      "tp_comm_bootstrap_backend: nccl\n",
      "use_cpu_initialization: None\n",
      "empty_unused_memory_level: 0\n",
      "deterministic_mode: False\n",
      "check_weight_hash_across_dp_replicas_interval: None\n",
      "calculate_per_token_loss: False\n",
      "train_sync_interval: None\n",
      "train_iters: 170005\n",
      "train_samples: None\n",
      "log_interval: 1\n",
      "exit_interval: None\n",
      "exit_duration_in_mins: None\n",
      "exit_signal_handler: False\n",
      "tensorboard_dir: /iopsstor/scratch/cscs/xyixuan/Megatron-LM/logs/Meg-Runs/Goldfish/llama3-1b-15n-8192sl-60gbsz-goldfish-no-bos/logging/tensorboard\n",
      "masked_softmax_fusion: True\n",
      "bias_gelu_fusion: False\n",
      "bias_swiglu_fusion: True\n",
      "bias_dropout_fusion: True\n",
      "apply_rope_fusion: True\n",
      "cross_entropy_loss_fusion: True\n",
      "use_flash_attn: False\n",
      "add_bias_linear: False\n",
      "add_qkv_bias: False\n",
      "optimizer: adam\n",
      "dataloader_type: single\n",
      "async_tensor_model_parallel_allreduce: True\n",
      "no_persist_layer_norm: False\n",
      "sequence_parallel: False\n",
      "gradient_accumulation_fusion: True\n",
      "deprecated_use_mcore_models: False\n",
      "use_legacy_models: False\n",
      "manual_gc: True\n",
      "manual_gc_interval: 5000\n",
      "manual_gc_eval: True\n",
      "tp_comm_split_ag: True\n",
      "tp_comm_split_rs: True\n",
      "seed: 28\n",
      "data_parallel_random_init: False\n",
      "init_method_std: 0.008944\n",
      "init_method_xavier_uniform: False\n",
      "lr: 0.0003\n",
      "lr_decay_style: cosine\n",
      "lr_wsd_decay_style: 1-sqrt\n",
      "lr_decay_iters: 170005\n",
      "lr_decay_samples: None\n",
      "lr_wsd_decay_samples: None\n",
      "lr_wsd_decay_iters: None\n",
      "lr_warmup_fraction: None\n",
      "lr_warmup_iters: 2000\n",
      "lr_warmup_samples: 0\n",
      "lr_warmup_init: 0.0\n",
      "min_lr: 3e-05\n",
      "override_opt_param_scheduler: False\n",
      "use_checkpoint_opt_param_scheduler: False\n",
      "decoupled_lr: None\n",
      "decoupled_min_lr: None\n",
      "save: /iopsstor/scratch/cscs/xyixuan/Megatron-LM/logs/Meg-Runs/Goldfish/llama3-1b-15n-8192sl-60gbsz-goldfish-no-bos/ckpt-torch/torch\n",
      "save_interval: 15455\n",
      "no_save_optim: None\n",
      "no_save_rng: None\n",
      "load: /iopsstor/scratch/cscs/xyixuan/Megatron-LM/logs/Meg-Runs/Goldfish/llama3-1b-15n-8192sl-60gbsz-goldfish-no-bos/checkpoints\n",
      "no_load_optim: None\n",
      "no_load_rng: None\n",
      "non_persistent_save_interval: None\n",
      "non_persistent_ckpt_type: None\n",
      "non_persistent_global_ckpt_dir: None\n",
      "non_persistent_local_ckpt_dir: None\n",
      "non_persistent_local_ckpt_algo: fully_parallel\n",
      "finetune: False\n",
      "pretrained_checkpoint: None\n",
      "ckpt_step: None\n",
      "perform_initialization: True\n",
      "use_checkpoint_args: False\n",
      "use_mp_args_from_checkpoint_args: False\n",
      "use_tokenizer_model_from_checkpoint_args: True\n",
      "exit_on_missing_checkpoint: True\n",
      "use_dist_ckpt_deprecated: False\n",
      "auto_detect_ckpt_format: False\n",
      "dist_ckpt_format_deprecated: None\n",
      "ckpt_format: torch\n",
      "ckpt_convert_format: torch\n",
      "ckpt_convert_save: /iopsstor/scratch/cscs/xyixuan/Megatron-LM/logs/Meg-Runs/Goldfish/llama3-1b-15n-8192sl-60gbsz-goldfish-no-bos/ckpt-torch\n",
      "ckpt_convert_update_legacy_dist_opt_format: False\n",
      "ckpt_fully_parallel_save_deprecated: False\n",
      "ckpt_fully_parallel_save: True\n",
      "async_save: None\n",
      "ckpt_fully_parallel_load: False\n",
      "ckpt_assume_constant_structure: False\n",
      "dist_ckpt_strictness: assume_ok_unexpected\n",
      "fp16: False\n",
      "bf16: True\n",
      "loss_scale: None\n",
      "initial_loss_scale: 4294967296\n",
      "min_loss_scale: 1.0\n",
      "loss_scale_window: 1000\n",
      "hysteresis: 2\n",
      "fp32_residual_connection: False\n",
      "apply_query_key_layer_scaling: False\n",
      "attention_softmax_in_fp32: False\n",
      "accumulate_allreduce_grads_in_fp32: False\n",
      "fp16_lm_cross_entropy: False\n",
      "tensor_model_parallel_size: 1\n",
      "encoder_tensor_model_parallel_size: 0\n",
      "pipeline_model_parallel_size: 1\n",
      "encoder_pipeline_model_parallel_size: 0\n",
      "pipeline_model_parallel_split_rank: None\n",
      "decoder_first_pipeline_num_layers: None\n",
      "decoder_last_pipeline_num_layers: None\n",
      "num_layers_per_virtual_pipeline_stage: None\n",
      "num_virtual_stages_per_pipeline_rank: None\n",
      "microbatch_group_size_per_vp_stage: None\n",
      "overlap_p2p_comm: False\n",
      "overlap_p2p_comm_warmup_flush: False\n",
      "distributed_backend: nccl\n",
      "distributed_timeout_minutes: 10\n",
      "overlap_grad_reduce: True\n",
      "defer_embedding_wgrad_compute: False\n",
      "wgrad_deferral_limit: 50\n",
      "align_grad_reduce: True\n",
      "ddp_bucket_size: None\n",
      "ddp_average_in_collective: False\n",
      "overlap_param_gather: True\n",
      "overlap_param_gather_with_optimizer_step: False\n",
      "align_param_gather: False\n",
      "scatter_gather_tensors_in_pipeline: True\n",
      "use_ring_exchange_p2p: False\n",
      "local_rank: 0\n",
      "lazy_mpu_init: None\n",
      "account_for_embedding_in_pipeline_split: False\n",
      "account_for_loss_in_pipeline_split: False\n",
      "use_distributed_optimizer: True\n",
      "num_distributed_optimizer_instances: 1\n",
      "use_torch_fsdp2: False\n",
      "context_parallel_size: 1\n",
      "cp_comm_type: ['p2p']\n",
      "hierarchical_context_parallel_sizes: None\n",
      "nccl_communicator_config_path: None\n",
      "use_tp_pp_dp_mapping: False\n",
      "replication: False\n",
      "replication_jump: None\n",
      "replication_factor: 2\n",
      "eval_iters: 100\n",
      "eval_interval: 1000\n",
      "test_mode: False\n",
      "skip_train: False\n",
      "data_path: ['/capstor/scratch/cscs/xyixuan/fineweb-edu-80B/finewebedu-llama3tok_text_document', '/iopsstor/scratch/cscs/xyixuan/dataset/sparse_gutenberg/rep_1/00000_tokens', '/iopsstor/scratch/cscs/xyixuan/dataset/sparse_gutenberg/rep_2/00000_tokens', '/iopsstor/scratch/cscs/xyixuan/dataset/sparse_gutenberg/rep_3/00000_tokens', '/iopsstor/scratch/cscs/xyixuan/dataset/sparse_gutenberg/rep_4/00000_tokens', '/iopsstor/scratch/cscs/xyixuan/dataset/sparse_gutenberg/rep_8/00000_tokens', '/iopsstor/scratch/cscs/xyixuan/dataset/sparse_gutenberg/rep_16/00000_tokens', '/iopsstor/scratch/cscs/xyixuan/dataset/sparse_gutenberg/rep_24/00000_tokens', '/iopsstor/scratch/cscs/xyixuan/dataset/sparse_gutenberg/rep_32/00000_tokens', '/iopsstor/scratch/cscs/xyixuan/dataset/sparse_gutenberg/rep_48/00000_tokens', '/iopsstor/scratch/cscs/xyixuan/dataset/sparse_gutenberg/rep_64/00000_tokens', '/iopsstor/scratch/cscs/xyixuan/dataset/sparse_gutenberg/rep_96/00000_tokens', '/iopsstor/scratch/cscs/xyixuan/dataset/sparse_gutenberg/rep_128/00000_tokens']\n",
      "split: 100,0,0\n",
      "train_data_path: None\n",
      "valid_data_path: None\n",
      "test_data_path: None\n",
      "data_args_path: None\n",
      "per_split_data_args_path: None\n",
      "data_cache_path: /iopsstor/scratch/cscs/xyixuan/datasets/cache\n",
      "mmap_bin_files: True\n",
      "mock_data: False\n",
      "seq_length: 8192\n",
      "encoder_seq_length: 8192\n",
      "decoder_seq_length: None\n",
      "retriever_seq_length: 256\n",
      "sample_rate: 1.0\n",
      "mask_prob: 0.15\n",
      "short_seq_prob: 0.1\n",
      "num_workers: 2\n",
      "reset_position_ids: False\n",
      "reset_attention_mask: True\n",
      "eod_mask_loss: False\n",
      "bod_hiding: True\n",
      "goldfish_loss: True\n",
      "goldfish_k: 50\n",
      "goldfish_h: 50\n",
      "create_attention_mask_in_dataloader: True\n",
      "cross_document_attention: True\n",
      "num_dataset_builder_threads: 1\n",
      "s3_cache_path: None\n",
      "vocab_size: None\n",
      "vocab_file: None\n",
      "merge_file: None\n",
      "vocab_extra_ids: 0\n",
      "tokenizer_type: HuggingFaceTokenizer\n",
      "tokenizer_model: nvidia/OpenMath2-Llama3.1-8B\n",
      "tiktoken_pattern: None\n",
      "tiktoken_num_special_tokens: 1000\n",
      "tiktoken_special_tokens: None\n",
      "adlr_autoresume: False\n",
      "adlr_autoresume_interval: 1000\n",
      "ict_head_size: None\n",
      "biencoder_projection_dim: 0\n",
      "biencoder_shared_query_context_model: False\n",
      "ict_load: None\n",
      "bert_load: None\n",
      "titles_data_path: None\n",
      "query_in_block_prob: 0.1\n",
      "use_one_sent_docs: False\n",
      "evidence_data_path: None\n",
      "retriever_report_topk_accuracies: []\n",
      "retriever_score_scaling: False\n",
      "block_data_path: None\n",
      "embedding_path: None\n",
      "indexer_batch_size: 128\n",
      "indexer_log_interval: 1000\n",
      "num_classes: 1000\n",
      "img_h: 224\n",
      "img_w: 224\n",
      "num_channels: 3\n",
      "patch_dim: 16\n",
      "classes_fraction: 1.0\n",
      "data_per_class_fraction: 1.0\n",
      "data_sharding: True\n",
      "head_lr_mult: 1.0\n",
      "vision_pretraining: False\n",
      "vision_pretraining_type: classify\n",
      "vision_backbone_type: vit\n",
      "swin_backbone_type: tiny\n",
      "mask_type: random\n",
      "mask_factor: 1.0\n",
      "iter_per_epoch: 1250\n",
      "dino_local_img_size: 96\n",
      "dino_local_crops_number: 10\n",
      "dino_head_hidden_size: 2048\n",
      "dino_bottleneck_size: 256\n",
      "dino_freeze_last_layer: 1\n",
      "dino_norm_last_layer: False\n",
      "dino_warmup_teacher_temp: 0.04\n",
      "dino_teacher_temp: 0.07\n",
      "dino_warmup_teacher_temp_epochs: 30\n",
      "qk_layernorm: False\n",
      "expert_model_parallel_size: 1\n",
      "expert_tensor_parallel_size: 1\n",
      "num_experts: None\n",
      "moe_layer_freq: 1\n",
      "moe_ffn_hidden_size: 8192\n",
      "moe_shared_expert_intermediate_size: None\n",
      "moe_shared_expert_overlap: False\n",
      "moe_grouped_gemm: False\n",
      "moe_router_load_balancing_type: aux_loss\n",
      "moe_router_score_function: softmax\n",
      "moe_router_topk: 2\n",
      "moe_router_pre_softmax: False\n",
      "moe_router_topk_limited_devices: None\n",
      "moe_router_topk_scaling_factor: None\n",
      "moe_router_enable_expert_bias: False\n",
      "moe_router_bias_update_rate: 0.001\n",
      "moe_use_legacy_grouped_gemm: False\n",
      "moe_aux_loss_coeff: 0.0\n",
      "moe_z_loss_coeff: None\n",
      "moe_input_jitter_eps: None\n",
      "moe_token_dispatcher_type: allgather\n",
      "moe_per_layer_logging: False\n",
      "moe_expert_capacity_factor: None\n",
      "moe_pad_expert_input_to_capacity: False\n",
      "moe_token_drop_policy: probs\n",
      "moe_layer_recompute: False\n",
      "moe_extended_tp: False\n",
      "moe_use_upcycling: False\n",
      "q_lora_rank: None\n",
      "kv_lora_rank: 32\n",
      "qk_head_dim: 128\n",
      "qk_pos_emb_head_dim: 64\n",
      "v_head_dim: 128\n",
      "rotary_scaling_factor: 1.0\n",
      "log_params_norm: False\n",
      "log_num_zeros_in_grad: False\n",
      "log_throughput: True\n",
      "log_progress: True\n",
      "timing_log_level: 0\n",
      "barrier_with_L1_time: True\n",
      "timing_log_option: minmax\n",
      "tensorboard_log_interval: 1\n",
      "tensorboard_queue_size: 1000\n",
      "log_timers_to_tensorboard: True\n",
      "log_loss_scale_to_tensorboard: False\n",
      "log_validation_ppl_to_tensorboard: False\n",
      "log_memory_to_tensorboard: True\n",
      "log_world_size_to_tensorboard: False\n",
      "wandb_project: Goldfish\n",
      "wandb_exp_name: llama3-1b-15n-8192sl-60gbsz-goldfish-no-bos-182467\n",
      "wandb_save_dir: /iopsstor/scratch/cscs/xyixuan/Megatron-LM/logs/Meg-Runs/Goldfish/llama3-1b-15n-8192sl-60gbsz-goldfish-no-bos/logging\n",
      "logging_level: None\n",
      "log_straggler: False\n",
      "disable_straggler_on_startup: False\n",
      "straggler_ctrlr_port: 65535\n",
      "straggler_minmax_count: 1\n",
      "inference_batch_times_seqlen_threshold: -1\n",
      "max_tokens_to_oom: 12000\n",
      "output_bert_embeddings: False\n",
      "bert_embedder_type: megatron\n",
      "flash_decode: False\n",
      "enable_cuda_graph: False\n",
      "cuda_graph_warmup_steps: 2\n",
      "inference_max_seq_length: 2560\n",
      "fp8: None\n",
      "fp8_margin: 0\n",
      "fp8_interval: 1\n",
      "fp8_amax_history_len: 1\n",
      "fp8_amax_compute_algo: most_recent\n",
      "fp8_wgrad: True\n",
      "transformer_impl: transformer_engine\n",
      "fp8_param_gather: False\n",
      "te_rng_tracker: False\n",
      "inference_rng_tracker: False\n",
      "retro_project_dir: None\n",
      "retro_add_retriever: False\n",
      "retro_cyclic_train_iters: None\n",
      "retro_encoder_layers: 2\n",
      "retro_encoder_hidden_dropout: 0.1\n",
      "retro_encoder_attention_dropout: 0.1\n",
      "retro_num_neighbors: 2\n",
      "retro_num_retrieved_chunks: 2\n",
      "retro_attention_gate: 1\n",
      "retro_verify_neighbor_count: True\n",
      "spec: None\n",
      "hybrid_attention_ratio: 0.0\n",
      "hybrid_mlp_ratio: 0.0\n",
      "hybrid_override_pattern: None\n",
      "yaml_cfg: None\n",
      "use_precision_aware_optimizer: True\n",
      "main_grads_dtype: torch.bfloat16\n",
      "main_params_dtype: torch.float32\n",
      "exp_avg_dtype: torch.float32\n",
      "exp_avg_sq_dtype: torch.float32\n",
      "enable_one_logger: True\n",
      "one_logger_project: megatron-lm\n",
      "one_logger_run_name: None\n",
      "one_logger_async: False\n",
      "app_tag_run_name: None\n",
      "app_tag_run_version: 0.0.0\n",
      "enable_ft_package: False\n",
      "calc_ft_timeouts: False\n",
      "config_logger_dir: \n",
      "error_injection_rate: 0\n",
      "error_injection_type: transient_error\n",
      "rerun_mode: disabled\n",
      "rank: 0\n",
      "world_size: 60\n",
      "use_dist_ckpt: False\n",
      "transformer_pipeline_model_parallel_size: 1\n",
      "data_parallel_size: 60\n",
      "virtual_pipeline_model_parallel_size: None\n",
      "params_dtype: torch.bfloat16\n",
      "consumed_train_samples: 10200300\n",
      "skipped_train_samples: 0\n",
      "consumed_valid_samples: 0\n",
      "variable_seq_lengths: False\n",
      "padded_vocab_size: 128256\n",
      "model_type: ModelType.encoder_or_decoder\n",
      "iteration: 170005\n",
      "num_floating_point_operations_so_far: 8.887287452918612e+20\n"
     ]
    }
   ],
   "source": [
    "args = checkpoint['args']\n",
    "for key, value in vars(args).items():\n",
    "   print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.hidden_size/args.num_attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['embedding.word_embeddings.weight', 'decoder.layers.0.self_attention.core_attention._extra_state', 'decoder.layers.0.self_attention.linear_proj.weight', 'decoder.layers.0.self_attention.linear_proj._extra_state', 'decoder.layers.0.self_attention.linear_qkv.layer_norm_weight', 'decoder.layers.0.self_attention.linear_qkv.weight', 'decoder.layers.0.self_attention.linear_qkv._extra_state', 'decoder.layers.0.mlp.linear_fc1.layer_norm_weight', 'decoder.layers.0.mlp.linear_fc1.weight', 'decoder.layers.0.mlp.linear_fc1._extra_state', 'decoder.layers.0.mlp.linear_fc2.weight', 'decoder.layers.0.mlp.linear_fc2._extra_state', 'decoder.layers.1.self_attention.core_attention._extra_state', 'decoder.layers.1.self_attention.linear_proj.weight', 'decoder.layers.1.self_attention.linear_proj._extra_state', 'decoder.layers.1.self_attention.linear_qkv.layer_norm_weight', 'decoder.layers.1.self_attention.linear_qkv.weight', 'decoder.layers.1.self_attention.linear_qkv._extra_state', 'decoder.layers.1.mlp.linear_fc1.layer_norm_weight', 'decoder.layers.1.mlp.linear_fc1.weight', 'decoder.layers.1.mlp.linear_fc1._extra_state', 'decoder.layers.1.mlp.linear_fc2.weight', 'decoder.layers.1.mlp.linear_fc2._extra_state', 'decoder.layers.2.self_attention.core_attention._extra_state', 'decoder.layers.2.self_attention.linear_proj.weight', 'decoder.layers.2.self_attention.linear_proj._extra_state', 'decoder.layers.2.self_attention.linear_qkv.layer_norm_weight', 'decoder.layers.2.self_attention.linear_qkv.weight', 'decoder.layers.2.self_attention.linear_qkv._extra_state', 'decoder.layers.2.mlp.linear_fc1.layer_norm_weight', 'decoder.layers.2.mlp.linear_fc1.weight', 'decoder.layers.2.mlp.linear_fc1._extra_state', 'decoder.layers.2.mlp.linear_fc2.weight', 'decoder.layers.2.mlp.linear_fc2._extra_state', 'decoder.layers.3.self_attention.core_attention._extra_state', 'decoder.layers.3.self_attention.linear_proj.weight', 'decoder.layers.3.self_attention.linear_proj._extra_state', 'decoder.layers.3.self_attention.linear_qkv.layer_norm_weight', 'decoder.layers.3.self_attention.linear_qkv.weight', 'decoder.layers.3.self_attention.linear_qkv._extra_state', 'decoder.layers.3.mlp.linear_fc1.layer_norm_weight', 'decoder.layers.3.mlp.linear_fc1.weight', 'decoder.layers.3.mlp.linear_fc1._extra_state', 'decoder.layers.3.mlp.linear_fc2.weight', 'decoder.layers.3.mlp.linear_fc2._extra_state', 'decoder.layers.4.self_attention.core_attention._extra_state', 'decoder.layers.4.self_attention.linear_proj.weight', 'decoder.layers.4.self_attention.linear_proj._extra_state', 'decoder.layers.4.self_attention.linear_qkv.layer_norm_weight', 'decoder.layers.4.self_attention.linear_qkv.weight', 'decoder.layers.4.self_attention.linear_qkv._extra_state', 'decoder.layers.4.mlp.linear_fc1.layer_norm_weight', 'decoder.layers.4.mlp.linear_fc1.weight', 'decoder.layers.4.mlp.linear_fc1._extra_state', 'decoder.layers.4.mlp.linear_fc2.weight', 'decoder.layers.4.mlp.linear_fc2._extra_state', 'decoder.layers.5.self_attention.core_attention._extra_state', 'decoder.layers.5.self_attention.linear_proj.weight', 'decoder.layers.5.self_attention.linear_proj._extra_state', 'decoder.layers.5.self_attention.linear_qkv.layer_norm_weight', 'decoder.layers.5.self_attention.linear_qkv.weight', 'decoder.layers.5.self_attention.linear_qkv._extra_state', 'decoder.layers.5.mlp.linear_fc1.layer_norm_weight', 'decoder.layers.5.mlp.linear_fc1.weight', 'decoder.layers.5.mlp.linear_fc1._extra_state', 'decoder.layers.5.mlp.linear_fc2.weight', 'decoder.layers.5.mlp.linear_fc2._extra_state', 'decoder.layers.6.self_attention.core_attention._extra_state', 'decoder.layers.6.self_attention.linear_proj.weight', 'decoder.layers.6.self_attention.linear_proj._extra_state', 'decoder.layers.6.self_attention.linear_qkv.layer_norm_weight', 'decoder.layers.6.self_attention.linear_qkv.weight', 'decoder.layers.6.self_attention.linear_qkv._extra_state', 'decoder.layers.6.mlp.linear_fc1.layer_norm_weight', 'decoder.layers.6.mlp.linear_fc1.weight', 'decoder.layers.6.mlp.linear_fc1._extra_state', 'decoder.layers.6.mlp.linear_fc2.weight', 'decoder.layers.6.mlp.linear_fc2._extra_state', 'decoder.layers.7.self_attention.core_attention._extra_state', 'decoder.layers.7.self_attention.linear_proj.weight', 'decoder.layers.7.self_attention.linear_proj._extra_state', 'decoder.layers.7.self_attention.linear_qkv.layer_norm_weight', 'decoder.layers.7.self_attention.linear_qkv.weight', 'decoder.layers.7.self_attention.linear_qkv._extra_state', 'decoder.layers.7.mlp.linear_fc1.layer_norm_weight', 'decoder.layers.7.mlp.linear_fc1.weight', 'decoder.layers.7.mlp.linear_fc1._extra_state', 'decoder.layers.7.mlp.linear_fc2.weight', 'decoder.layers.7.mlp.linear_fc2._extra_state', 'decoder.layers.8.self_attention.core_attention._extra_state', 'decoder.layers.8.self_attention.linear_proj.weight', 'decoder.layers.8.self_attention.linear_proj._extra_state', 'decoder.layers.8.self_attention.linear_qkv.layer_norm_weight', 'decoder.layers.8.self_attention.linear_qkv.weight', 'decoder.layers.8.self_attention.linear_qkv._extra_state', 'decoder.layers.8.mlp.linear_fc1.layer_norm_weight', 'decoder.layers.8.mlp.linear_fc1.weight', 'decoder.layers.8.mlp.linear_fc1._extra_state', 'decoder.layers.8.mlp.linear_fc2.weight', 'decoder.layers.8.mlp.linear_fc2._extra_state', 'decoder.layers.9.self_attention.core_attention._extra_state', 'decoder.layers.9.self_attention.linear_proj.weight', 'decoder.layers.9.self_attention.linear_proj._extra_state', 'decoder.layers.9.self_attention.linear_qkv.layer_norm_weight', 'decoder.layers.9.self_attention.linear_qkv.weight', 'decoder.layers.9.self_attention.linear_qkv._extra_state', 'decoder.layers.9.mlp.linear_fc1.layer_norm_weight', 'decoder.layers.9.mlp.linear_fc1.weight', 'decoder.layers.9.mlp.linear_fc1._extra_state', 'decoder.layers.9.mlp.linear_fc2.weight', 'decoder.layers.9.mlp.linear_fc2._extra_state', 'decoder.layers.10.self_attention.core_attention._extra_state', 'decoder.layers.10.self_attention.linear_proj.weight', 'decoder.layers.10.self_attention.linear_proj._extra_state', 'decoder.layers.10.self_attention.linear_qkv.layer_norm_weight', 'decoder.layers.10.self_attention.linear_qkv.weight', 'decoder.layers.10.self_attention.linear_qkv._extra_state', 'decoder.layers.10.mlp.linear_fc1.layer_norm_weight', 'decoder.layers.10.mlp.linear_fc1.weight', 'decoder.layers.10.mlp.linear_fc1._extra_state', 'decoder.layers.10.mlp.linear_fc2.weight', 'decoder.layers.10.mlp.linear_fc2._extra_state', 'decoder.layers.11.self_attention.core_attention._extra_state', 'decoder.layers.11.self_attention.linear_proj.weight', 'decoder.layers.11.self_attention.linear_proj._extra_state', 'decoder.layers.11.self_attention.linear_qkv.layer_norm_weight', 'decoder.layers.11.self_attention.linear_qkv.weight', 'decoder.layers.11.self_attention.linear_qkv._extra_state', 'decoder.layers.11.mlp.linear_fc1.layer_norm_weight', 'decoder.layers.11.mlp.linear_fc1.weight', 'decoder.layers.11.mlp.linear_fc1._extra_state', 'decoder.layers.11.mlp.linear_fc2.weight', 'decoder.layers.11.mlp.linear_fc2._extra_state', 'decoder.layers.12.self_attention.core_attention._extra_state', 'decoder.layers.12.self_attention.linear_proj.weight', 'decoder.layers.12.self_attention.linear_proj._extra_state', 'decoder.layers.12.self_attention.linear_qkv.layer_norm_weight', 'decoder.layers.12.self_attention.linear_qkv.weight', 'decoder.layers.12.self_attention.linear_qkv._extra_state', 'decoder.layers.12.mlp.linear_fc1.layer_norm_weight', 'decoder.layers.12.mlp.linear_fc1.weight', 'decoder.layers.12.mlp.linear_fc1._extra_state', 'decoder.layers.12.mlp.linear_fc2.weight', 'decoder.layers.12.mlp.linear_fc2._extra_state', 'decoder.layers.13.self_attention.core_attention._extra_state', 'decoder.layers.13.self_attention.linear_proj.weight', 'decoder.layers.13.self_attention.linear_proj._extra_state', 'decoder.layers.13.self_attention.linear_qkv.layer_norm_weight', 'decoder.layers.13.self_attention.linear_qkv.weight', 'decoder.layers.13.self_attention.linear_qkv._extra_state', 'decoder.layers.13.mlp.linear_fc1.layer_norm_weight', 'decoder.layers.13.mlp.linear_fc1.weight', 'decoder.layers.13.mlp.linear_fc1._extra_state', 'decoder.layers.13.mlp.linear_fc2.weight', 'decoder.layers.13.mlp.linear_fc2._extra_state', 'decoder.layers.14.self_attention.core_attention._extra_state', 'decoder.layers.14.self_attention.linear_proj.weight', 'decoder.layers.14.self_attention.linear_proj._extra_state', 'decoder.layers.14.self_attention.linear_qkv.layer_norm_weight', 'decoder.layers.14.self_attention.linear_qkv.weight', 'decoder.layers.14.self_attention.linear_qkv._extra_state', 'decoder.layers.14.mlp.linear_fc1.layer_norm_weight', 'decoder.layers.14.mlp.linear_fc1.weight', 'decoder.layers.14.mlp.linear_fc1._extra_state', 'decoder.layers.14.mlp.linear_fc2.weight', 'decoder.layers.14.mlp.linear_fc2._extra_state', 'decoder.layers.15.self_attention.core_attention._extra_state', 'decoder.layers.15.self_attention.linear_proj.weight', 'decoder.layers.15.self_attention.linear_proj._extra_state', 'decoder.layers.15.self_attention.linear_qkv.layer_norm_weight', 'decoder.layers.15.self_attention.linear_qkv.weight', 'decoder.layers.15.self_attention.linear_qkv._extra_state', 'decoder.layers.15.mlp.linear_fc1.layer_norm_weight', 'decoder.layers.15.mlp.linear_fc1.weight', 'decoder.layers.15.mlp.linear_fc1._extra_state', 'decoder.layers.15.mlp.linear_fc2.weight', 'decoder.layers.15.mlp.linear_fc2._extra_state', 'decoder.final_layernorm.weight', 'decoder.final_layernorm._extra_state', 'output_layer._extra_state'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['model'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QKV weight shape: torch.Size([3072, 2048])\n",
      "QKV weight dtype: torch.bfloat16\n",
      "Embedding shape: torch.Size([128256, 2048])\n"
     ]
    }
   ],
   "source": [
    "# Check hidden size\n",
    "qkv_weight = checkpoint['model']['decoder.layers.0.self_attention.linear_qkv.weight']\n",
    "print(f\"QKV weight shape: {qkv_weight.shape}\")\n",
    "\n",
    "# Check dtype\n",
    "print(f\"QKV weight dtype: {qkv_weight.dtype}\")\n",
    "\n",
    "# Check embedding size\n",
    "emb_weight = checkpoint['model']['embedding.word_embeddings.weight']\n",
    "print(f\"Embedding shape: {emb_weight.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
